{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Wrangling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cl_train = pd.read_csv(os.path.join(\"dataset\", \"train.csv\"))\n",
    "cl_test = pd.read_csv(os.path.join(\"dataset\", \"test.csv\"))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "cl_train.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                             excerpt    target  standard_error  \n",
       "0  When the young people returned to the ballroom... -0.340259        0.464009  \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n",
       "3  And outside before the palace a great garden w... -1.054013        0.450007  \n",
       "4  Once upon a time there were Three Bears who li...  0.247197        0.510845  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It looks like we could only use one predictor `excerpt` to predict `target`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use PorterStemmer to stem the sentences in the dataset which strips a suffix of words. LancasterStemmer is dropped because the stemming algorithm is too aggressive causing over-stemming."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    \"\"\" Given a sentence,\n",
    "    modify each word in the sentence to stemmed word.\n",
    "    \"\"\"\n",
    "    porter = PorterStemmer()\n",
    "    words = word_tokenize(sentence)\n",
    "    stemmed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        stemmed_words.append(porter.stem(word))\n",
    "        stemmed_words.append(\" \")\n",
    "\n",
    "    return \"\".join(stemmed_words)\n",
    "\n",
    "def stem_paragraph(paragraph):\n",
    "    \"\"\" Given a paragraph\n",
    "    return a paragraph whose word is stemmed.\n",
    "    \"\"\"\n",
    "    stemmed_sentence = []\n",
    "    for sentence in paragraph.split(\"\\n\"):\n",
    "        stemmed = stem_sentence(sentence)\n",
    "        stemmed_sentence.append(stemmed)\n",
    "        stemmed_sentence.append(\"\\n\")\n",
    "\n",
    "    return \"\".join(stemmed_sentence)\n",
    "\n",
    "def stem_dataset(dataset):\n",
    "    # Stem all paragraphs in the dataset\n",
    "    dataset_copy = dataset.copy()\n",
    "    \n",
    "    for index, row in dataset_copy.iterrows():\n",
    "        dataset_copy.loc[index, 'clean_excerpt'] = stem_paragraph(row['excerpt'])\n",
    "\n",
    "    return dataset_copy\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_X(clean_data):\n",
    "    vectorizer = TfidfVectorizer(lowercase=True,token_pattern=r'(?u)\\b[A-Za-z]+\\b',stop_words='english',max_features=2000,strip_accents='unicode')\n",
    "    vectorizer.fit(clean_data['clean_excerpt'].values)\n",
    "\n",
    "    X = pd.DataFrame(columns= range(0, 2000))\n",
    "\n",
    "    for index, row in clean_data.iterrows():\n",
    "        numbers = vectorizer.transform(clean_data['clean_excerpt'][[index]])\n",
    "        X = X.append(pd.DataFrame(numbers.toarray()))\n",
    "\n",
    "    return X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Selection"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we will explore several ML models to fit the data.\n",
    "We use k-fold cross validation to determine the (negative) Mean Squared Prediction Error.\n",
    "For evalution of models, I will use cross-validation rather than hold-out methods because the data is not too large and the trend of data might change drastically depending on how the data is split. Also, I'd like to use all the training data to fit a model which cannot be done with hold-out method."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def summary_scores(scores):\n",
    "    print(\"scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"StdDev\", scores.std())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "y = cl_train.loc[:, 'target']\n",
    "clean_train = stem_dataset(cl_train)\n",
    "X = get_X(clean_train)\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "scores = cross_val_score(forest_reg, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "summary_scores(scores)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "scores: [-0.5494264  -0.77342274 -0.76808348 -0.81752546 -0.71320759 -0.81412132\n",
      " -0.92116885 -0.89019521 -0.49008581 -0.74388369]\n",
      "Mean: -0.7481120536214574\n",
      "StdDev 0.1294762322116122\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XGBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_reg = XGBRegressor()\n",
    "scores = cross_val_score(xgb_reg, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "summary_scores(scores)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "scores: [-0.60714023 -0.76548463 -0.72409527 -0.72086336 -0.66467657 -0.73704524\n",
      " -0.84123137 -0.87512155 -0.60346959 -0.71691881]\n",
      "Mean: -0.725604660725046\n",
      "StdDev 0.08379102854461291\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elastic Net"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "scores = cross_val_score(elastic_net, X, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "summary_scores(scores)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "scores: [-0.59897904 -0.94855322 -1.0715376  -1.31934371 -1.08471865 -1.32741218\n",
      " -1.66454245 -1.19868725 -0.53320022 -1.1841688 ]\n",
      "Mean: -1.093114312118898\n",
      "StdDev 0.3211151385475014\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "Looks like random forest regressor and XGBoost seem very promising. We will try hypertune these models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## hyperparameter tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid_rf = [\n",
    "    {'n_estimators': [3, 10, 30, 50, 100], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap':[False], 'n_estimators': [3,10], 'max_features': [2, 3, 4]}\n",
    "]\n",
    "\n",
    "grid_search_rf = GridSearchCV(forest_reg, param_grid_rf, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search_rf.fit(X, y)\n",
    "result = grid_search_rf.cv_results_\n",
    "for mean_score, params in zip(result[\"mean_test_score\"], result[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1.1272475484489852 {'max_features': 2, 'n_estimators': 3}\n",
      "-0.8493934919743149 {'max_features': 2, 'n_estimators': 10}\n",
      "-0.7845822072537697 {'max_features': 2, 'n_estimators': 30}\n",
      "-0.7791184846199336 {'max_features': 2, 'n_estimators': 50}\n",
      "-0.7728278298944244 {'max_features': 2, 'n_estimators': 100}\n",
      "-1.0548520213893688 {'max_features': 4, 'n_estimators': 3}\n",
      "-0.855060500597002 {'max_features': 4, 'n_estimators': 10}\n",
      "-0.7809480578570392 {'max_features': 4, 'n_estimators': 30}\n",
      "-0.7683194080375831 {'max_features': 4, 'n_estimators': 50}\n",
      "-0.751077660763361 {'max_features': 4, 'n_estimators': 100}\n",
      "-1.0575076450300147 {'max_features': 6, 'n_estimators': 3}\n",
      "-0.8185961891621819 {'max_features': 6, 'n_estimators': 10}\n",
      "-0.7644292597009067 {'max_features': 6, 'n_estimators': 30}\n",
      "-0.7464606903932263 {'max_features': 6, 'n_estimators': 50}\n",
      "-0.7450545822667392 {'max_features': 6, 'n_estimators': 100}\n",
      "-1.005575226272889 {'max_features': 8, 'n_estimators': 3}\n",
      "-0.8279898384970001 {'max_features': 8, 'n_estimators': 10}\n",
      "-0.7618779785682632 {'max_features': 8, 'n_estimators': 30}\n",
      "-0.7457971802960145 {'max_features': 8, 'n_estimators': 50}\n",
      "-0.7412919413472446 {'max_features': 8, 'n_estimators': 100}\n",
      "-1.0920345265167852 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "-0.8284335156428622 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "-1.0553669854302823 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "-0.8334762748554126 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "-1.0256580660971661 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "-0.8243230816649574 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "param_grid_xgb = [\n",
    "    {'max_depth': [2, 3, 4, 5, 6], 'learning_rate': [0.1, 0.2, 0.3, 0.5, 0.8]}\n",
    "]\n",
    "\n",
    "grid_search_xgb = GridSearchCV(xgb_reg, param_grid_xgb, cv=10, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "grid_search_xgb.fit(X, y)\n",
    "result = grid_search_xgb.cv_results_\n",
    "result = grid_search_xgb.cv_results_\n",
    "for mean_score, params in zip(result[\"mean_test_score\"], result[\"params\"]):\n",
    "    print(mean_score, params)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-0.7858697012544287 {'learning_rate': 0.1, 'max_depth': 2}\n",
      "-0.7491335550807847 {'learning_rate': 0.1, 'max_depth': 3}\n",
      "-0.7244502287552528 {'learning_rate': 0.1, 'max_depth': 4}\n",
      "-0.7131853128195291 {'learning_rate': 0.1, 'max_depth': 5}\n",
      "-0.7065250615087468 {'learning_rate': 0.1, 'max_depth': 6}\n",
      "-0.7225685660193695 {'learning_rate': 0.2, 'max_depth': 2}\n",
      "-0.700593077063764 {'learning_rate': 0.2, 'max_depth': 3}\n",
      "-0.6971121954140018 {'learning_rate': 0.2, 'max_depth': 4}\n",
      "-0.6972089150438523 {'learning_rate': 0.2, 'max_depth': 5}\n",
      "-0.6985663051005833 {'learning_rate': 0.2, 'max_depth': 6}\n",
      "-0.7100671022445013 {'learning_rate': 0.3, 'max_depth': 2}\n",
      "-0.7078963752420718 {'learning_rate': 0.3, 'max_depth': 3}\n",
      "-0.7092652813677149 {'learning_rate': 0.3, 'max_depth': 4}\n",
      "-0.7185644111652352 {'learning_rate': 0.3, 'max_depth': 5}\n",
      "-0.725604660725046 {'learning_rate': 0.3, 'max_depth': 6}\n",
      "-0.7564854851517179 {'learning_rate': 0.5, 'max_depth': 2}\n",
      "-0.759832062530283 {'learning_rate': 0.5, 'max_depth': 3}\n",
      "-0.7933003715202873 {'learning_rate': 0.5, 'max_depth': 4}\n",
      "-0.8276965375282895 {'learning_rate': 0.5, 'max_depth': 5}\n",
      "-0.7958166609616262 {'learning_rate': 0.5, 'max_depth': 6}\n",
      "-0.8837127177384421 {'learning_rate': 0.8, 'max_depth': 2}\n",
      "-0.949935199814413 {'learning_rate': 0.8, 'max_depth': 3}\n",
      "-0.9814942503055735 {'learning_rate': 0.8, 'max_depth': 4}\n",
      "-1.0406027370804622 {'learning_rate': 0.8, 'max_depth': 5}\n",
      "-1.0434246607150535 {'learning_rate': 0.8, 'max_depth': 6}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "I have decided to use XG boost over random forest."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "import pickle\n",
    "\n",
    "final_model =grid_search_xgb.best_estimator_\n",
    "\n",
    "pickle.dump(final_model, open(os.path.join(\"models\", \"xgb_final.sav\"), 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submission"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "X_test = get_X(stem_dataset(cl_test))\n",
    "final_prediction = final_model.predict(X_test)\n",
    "fp = pd.DataFrame(final_prediction, columns=['target'])\n",
    "\n",
    "output = cl_test[['id']].join(fp)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('ml_env': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "interpreter": {
   "hash": "510aca93a48ec1fac3ba8e0a8ff62e3270bf536b41a9de8e400cc6cb2edcbd11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}