{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\r\n",
    "import torch\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from transformers import AutoConfig, AutoTokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Exploration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dftrain = pd.read_csv('datasets/train.csv')\r\n",
    "dftest = pd.read_csv('datasets/test.csv')\r\n",
    "\r\n",
    "trainset, valset = train_test_split(dftrain, test_size=0.2, random_state=69)\r\n",
    "trainset = trainset.reset_index()\r\n",
    "valset = valset.reset_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_counts = dftrain['excerpt'].apply(lambda x: len(x.split()))\r\n",
    "print(f'max number of words: {word_counts.max()}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The max number of words is 205 so this can be the indicator of number of the tokenised text."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up functions and datasets to train a model.\r\n",
    "We will be training pretrained BERT model from huggingface library"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\r\n",
    "from transformers import AutoConfig, AutoTokenizer\r\n",
    "\r\n",
    "class BertRegresser(BertPreTrainedModel):\r\n",
    "    def __init__(self, config):\r\n",
    "        super().__init__(config)\r\n",
    "        self.bert = BertModel(config)\r\n",
    "        #The output layer that takes the [CLS] representation and gives an output\r\n",
    "        self.cls_layer1 = nn.Linear(config.hidden_size,128)\r\n",
    "        self.relu1 = nn.ReLU()\r\n",
    "        self.ff1 = nn.Linear(128,128)\r\n",
    "        self.tanh1 = nn.Tanh()\r\n",
    "        self.ff2 = nn.Linear(128,1)\r\n",
    "\r\n",
    "    def forward(self, input_ids, attention_mask):\r\n",
    "        #Feed the input to Bert model to obtain contextualized representations\r\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\r\n",
    "        #Obtain the representations of [CLS] heads\r\n",
    "        logits = outputs.last_hidden_state[:,0,:]\r\n",
    "        output = self.cls_layer1(logits)\r\n",
    "        output = self.relu1(output)\r\n",
    "        output = self.ff1(output)\r\n",
    "        output = self.tanh1(output)\r\n",
    "        output = self.ff2(output)\r\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Dataset(torch.utils.data.Dataset):\r\n",
    "    def __init__(self, data, maxlen, tokenizer):\r\n",
    "        self.df = data\r\n",
    "        self.tokenizer = tokenizer\r\n",
    "        self.maxlen = maxlen\r\n",
    "    def __len__(self):\r\n",
    "        return self.df.shape[0]\r\n",
    "\r\n",
    "    def __getitem__(self, i):\r\n",
    "        # input ids, attention mask and label for that index i.\r\n",
    "        excerpt = self.df.loc[i, 'excerpt']\r\n",
    "        target = self.df.loc[i, 'target']\r\n",
    "        tokens = self.tokenizer(excerpt, max_length = self.maxlen, padding='max_length', truncation=True, return_tensors='pt')\r\n",
    "\r\n",
    "        target = torch.tensor(target, dtype=torch.float32)\r\n",
    "\r\n",
    "        return tokens['input_ids'].squeeze(0), tokens['attention_mask'], target"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from transformers import AdamW\r\n",
    "import torch.nn as nn\r\n",
    "from tqdm import tqdm, trange\r\n",
    "def train(model, train_loader, val_loader, epochs, device):\r\n",
    "    criterion = nn.MSELoss()\r\n",
    "    best_acc = 0\r\n",
    "    model.train()\r\n",
    "    optim = AdamW(model.parameters(), lr=1e-3)\r\n",
    "    for epoch in trange(epochs, desc=\"Epoch\"):\r\n",
    "        train_loss = 0\r\n",
    "        for i, (input_ids, attention_mask, target) in enumerate(iterable=train_loader):\r\n",
    "            optim.zero_grad()\r\n",
    "            input_ids = input_ids.to(device)\r\n",
    "            attention_mask = attention_mask.to(device)\r\n",
    "            target = target.to(device)\r\n",
    "            \r\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask)\r\n",
    "            \r\n",
    "            loss = criterion(outputs, target.type_as(outputs))\r\n",
    "            loss.backward()\r\n",
    "            optim.step()\r\n",
    "\r\n",
    "            train_loss += loss.item()\r\n",
    "        \r\n",
    "        print(f'Training loss: {train_loss/len(train_loader)}')\r\n",
    "        val_loss = evaluate(model, val_loader, device)\r\n",
    "        print(f'epoch :{epoch} Val loss: {val_loss}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluate(model, val_loader, device):\r\n",
    "    model.eval()\r\n",
    "    total_loss, count = 0, 0\r\n",
    "    criterion = nn.MSELoss()\r\n",
    "    with torch.no_grad():\r\n",
    "        for input_ids, attention_mask, target in (val_loader):\r\n",
    "            input_ids = input_ids.to(device)\r\n",
    "            attention_mask = attention_mask.to(device)\r\n",
    "            target = target.to(device)\r\n",
    "\r\n",
    "            outputs = model(input_ids, attention_mask)\r\n",
    "            total_loss += criterion(outputs, target.type_as(outputs)).item()\r\n",
    "            count += 1\r\n",
    "    \r\n",
    "    return total_loss/count"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")\r\n",
    "model = BertRegresser.from_pretrained(\"bert-base-uncased\", config = config)\r\n",
    "model = model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_dataset = Dataset(data=trainset, maxlen=205, tokenizer = tokenizer)\r\n",
    "val_dataset = Dataset(data=valset, maxlen=205, tokenizer = tokenizer)\r\n",
    "\r\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\r\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train(model, train_loader, val_loader, 10, device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, I could not finish training the model with the computation power I have. I have tried to decrease the learning rate and using larger batches but it didn't seem to work."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit ('msa': venv)"
  },
  "interpreter": {
   "hash": "66d534e1a2c2826439d9610ae91462fc6a36137a56de058f5776b53ead75132d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}